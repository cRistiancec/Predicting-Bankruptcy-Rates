---
title: "Final Method chosen"
author: "Evelyn Peng"
date: "November 27, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(ggplot2)
library("tseries")
library(forecast)
library(lawstat)
```


```{r cars}
df <- read.csv("train.csv")
time_s <- df %>% dplyr::select(Bankruptcy_Rate) %>% ts()

##re-format date so we cna plot using ggplot
df$Date <-zoo::as.Date(zoo::as.yearmon(sprintf("%06s",as.character(df$Month)), "%m%Y"))

ggplot(df, aes(Date, Bankruptcy_Rate)) + geom_line()  + xlab("") + ylab("Bankruptcy Rates") + ggtitle("Canadian Bankruptcy Rates ") + geom_line(aes(Date, Unemployment_Rate))
```

Plot time series data and ACF to see if the data is stationary. 

```{r}
par(mfrow=c(2,1))
plot(df$Bankruptcy_Rate)
acf(df$Bankruptcy_Rate, lag.max = 48)
adf.test(df$Bankruptcy_Rate)
```

We found the ACF showing undesirable slow decay indicating the data has a trend. And formal test, Augmented Dicky-Fuller, also support this observation with p-value = 0.3859. We should not reject the null hypothesis that the time series data is not stationary. To stable the time series, we begin with ordinary difference to minimize the trend. We then difference the data until the transformed time series looks flat.

```{r}
# The raw time series is clearly not stationary. Try differencing once:
BR1 <- diff(df$Bankruptcy_Rate)
plot(BR1, ylab = "BR1")
acf(BR1, lag.max = 48)
adf.test(BR1)
```

The Bankruptcy Rate passed Dickey-Fuller test after difference once. The data is now stationary without trend and we need to eliminate seasonality next. 

```{r}
# There still seems to be monthly seasonality (period = 12). Let's try differencing for that.
BR1.12 <- diff(BR1, lag = 12)
plot(BR1.12)
acf(BR1.12, lag.max = 48)
```

ACF plots looks better after seasonal difference once. The peaks between two cycle decay rapidly. We then use auto.arima to check if using library will give the same result. 

```{r}
auto.arima(df$Bankruptcy_Rate, allowdrift = F)
nsdiffs(df$Bankruptcy_Rate, m=12)
```


Even though auto.arima diagnose the data should perform without seasonality, we could still see a clear seasonal cycle in the data. We will process with ordinary difference with once and seasonal difference with twice. From the acf plot of the final different-ed data, the candidate q is between 1 and 7. The possible range of p is 1 and 8. And P, Q both could fall in 0 to 2.

```{r}
par(mfrow=c(2,1))
acf(BR1.12, lag.max = 48)
pacf(BR1.12, lag.max=48)

AIC <- 9999999
sigma <- 9999999
final <- list(0,0,0,0)

# for (p in 0:5){
#   for (q in 0:7){
#     for (P in 0:2){
#       for (Q in 0:2){
#         try({
#           m.ml <- arima(df$Bankruptcy_Rate, order = c(p,1,q), seasonal = list(order = c(P,1,Q), period = 12), method = "CSS-ML")
#           cat(sprintf("\"%d\" \"%d\" \"%d\ \"%d\ \n", p, q, P, Q))
#           print(m.ml$aic)
#           if (m.ml$aic < AIC) {
#             AIC <- m.ml$aic
#             final[1] <- p
#             final[2] <- q
#             final[3] <- P
#             final[4] <- Q
#           }})
#       }
#     }
#   }
# }

```

(Using nested for loop) [should not included in the final report], (p,q,P,Q) with value (5,5,2,2) has the lowest AIC value, this servers as our first candidate model. And (5, 6, 2, 1) server as our second candidate model. 

```{r}
m1 <- arima(df$Bankruptcy_Rate, order = c(5,1,5), seasonal = list(order = c(2,1,2), period = 12), method = "CSS-ML")
m2 <- arima(df$Bankruptcy_Rate, order = c(5,1,6), seasonal = list(order = c(2,1,1), period = 12), method = "CSS-ML")
m1
m2
```


|                  | (5,1,5) * (2,1,2) | (5,1,6) * (2,1,1)  |
|------------------|--------|--------|
| AIC              | -2774.04 | -2773.61 |
| log likelihood | 1402.02 | 1401.81 |

From the comparison table, we could clearly confirm the first model perform better than the second one. We therefore choose the parameter of arima model (p,q,P,Q) accordingly. With the candidate model arima (5,5,2,2), we start checking if it satisfy the formal and informal residual diagnostics.

i. Zero-Mean

```{r}
e <- m1$residuals # residuals
r <- e/sqrt(m1$sigma2) # standardized residuals 
par(mfrow=c(2,1))
plot(e, main="Residuals vs t", ylab="")
abline(h=0, col="red")
plot(r, main="Standardized Residuals vs t", ylab="") 
abline(h=0, col="red")

# test whether residuals have zero mean
test <- t.test(e) # do ont reject null hypothese, the mean is 0
print(c("Test Statistic:",round(test$statistic,4),"P-value:",round(test$p.value,4)))
```

From the plot of standardized residuals and time t, we see no obvious above or below 0. And the one sample t-test gave p-value = 0.8953 indicating we should not reject the null hypotheses. The true mean is equal to 0 with more than 95 percent confidence level. 

ii. Homoscedasticity

```{r}
# test for heteroscedasticity
par(mfrow=c(1,1))
plot(e, main="Residuals vs t", ylab="")
#group the data, to see if the variance is the same
abline(v=150, lwd=3, col="red")
#group the data, to see if the variance is the same
group <- c(rep(1,150),rep(2,138))
test <- levene.test(e,group)
print(c("Test Statistic:",round(test$statistic,4),"P-value:",round(test$p.value,4)))
#Levene rejects null hypothesis, h0 = all variance are not the same
test <- bartlett.test(e,group) #Bartlett
print(c("Test Statistic:",round(test$statistic,4),"P-value:",round(test$p.value,4)))
```

From the plot of standardized residuals and time t, we see slightly differences variance between groups. I divided the group by 3, each group last for around 100. The variance do not stay constant for all the groups. Levene test gave p-value = 0.0069 and Bartlett test gave p-value more than 0.0016, indicating we should reject the null hypothesis that all variance are the same across groups. However, the Bartlett test is sensitive to even slightly deviation from normal distribution, we should not trust the p-value. And the levene test gave a p-value not really small. Therefore, even though this model fails to pass leven test, the heteroskedasticity in this model is not too obvious. 

iii. Zero-Correlation 

```{r}
# test for uncorrelatedness / randomness
tsdiag(m1, gof.lag = 45)
```

The function tsdiag give the graph of ACF and Ljung-Box test all in one! From the ACF plots, it shows the residuals' are uncorrelated (only one spikes at 0 and no spikes afterwards) and for formal test of correlation, Ljung-Box test, all the p-value are larger than the critical value and we should not reject the null hypothesis that all correlations are equal to 0 because all the p value is above the confidence interval. The residuals do not have correlation for all lags.

iv. Normality

```{r}
# test for normality
par(mfrow=c(1,1))
qqnorm(e, main="QQ-plot of Residuals") #seems quite good
qqline(e, col = "red")
test <- shapiro.test(e) #SW test, reject null
print(c("Test Statistic:",round(test$statistic,4),"P-value:",round(test$p.value,4)))
```

The qqplot seems quite good, the empirical dots lies on the theoretical normal distribution line. Although the formal test for normality-Shapiro test, gave relatively small p value suggesting we should reject the null hypothesis, the visualization showing the residues are quite normally distributed. The normality test is not useful when dealing with real life problem. No real quantity is exactly normally distributed. The normal distribution is just a mathematical abstraction thatâ€™s a good enough approximation in a lot of cases. Therefore, we should conclude our data is normal distributed enough by observing qqplot.

## Exponential smoothing (Holt-Winters Methods)
From previous diagnosis, we found there are trend and seasonality in these data. Therefore, we adapt Triple Exponential Something method with multiple effect (apply on heteroskedastic data). However, after trying four different types of Holt-Winter method, there is no sign of seasonality. We cannot use Exponential Smoothing to predict future bankruptcy rates.

```{r}
par(mfrow = c(2,1))
plot(forecast(HoltWinters(x = df$Bankruptcy_Rate, beta = F, gamma = F), h = 60)) # no trend no seasonal
plot(forecast(HoltWinters(x = df$Bankruptcy_Rate, gamma = F), h = 60)) # no seasonal
# plot(forecast(HoltWinters(x = df$Bankruptcy_Rate, seasonal = "add"), h = 60)) 
# plot(forecast(HoltWinters(x = df$Bankruptcy_Rate, seasonal = "mult"), h = 60)) # is better then additive becase it increase the varibility with time
```


```{r}
# Triple Exponential Smoothing -- Multiplicative
# par(mfrow = c(1,1))
# plot(AirPassengers, main = "National Bankruptcy rates", ylab = "Bankruptcy Rate", xlab = "Time")
# hw.AP <- HoltWinters(x = df$Bankruptcy_Rate, seasonal = "mult") 
# par(mfrow = c(2,1))
# plot(hw.AP)
# plot(forecast(hw.AP, h = 60))
```



